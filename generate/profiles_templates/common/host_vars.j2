---
# Kubernetes node configuration
# Do not change profile_name, configured_nic and configured_arch here !!!
# To generate vars for different profile/architecture use make command
# generated for profile and arch:
profile_name: {{ name }}
configured_arch: {{ arch }}
configured_nic: {{ nic }}

{% if sriov_operator in ['on', 'optional'] or sriov_network_dp in ['on', 'optional'] or qat in ['on', 'optional'] -%}
# Enable IOMMU (required for SR-IOV networking and QAT)
iommu_enabled: {% if (sriov_operator == 'on' or sriov_network_dp == 'on' or qat == 'on') and on_vms != 'on' %}true{% else %}false{% endif %}
{% endif %}

{%- if nic_drivers in ['on', 'optional'] %}
# Set to 'true' to update i40e, ice and iavf kernel modules
update_nic_drivers: {% if nic_drivers == 'on' %}true{% else %}false{% endif %}
# Set 'true' to update NIC firmware
update_nic_firmware: false # FW update will be executed on all NICs listed in "dataplane_interfaces[*].name"

# dataplane interface configuration list
dataplane_interfaces: []
{%- if on_vms == 'on' %}
#  - bus_info: "06:00.0"             # pci bus info
#    pf_driver: iavf              # driver inside VM
#    sriov_numvfs: 0
#    default_vf_driver: "igb_uio"
#  - bus_info: "07:00.0"             # pci bus info
#    pf_driver: iavf              # driver inside VM
#    sriov_numvfs: 0
#    default_vf_driver: "iavf"
#  - bus_info: "08:00.0"             # pci bus info
#    pf_driver: iavf              # driver inside VM
#    sriov_numvfs: 0
#    default_vf_driver: "iavf"
#  - bus_info: "09:00.0"             # pci bus info
#    pf_driver: iavf              # driver inside VM
#    sriov_numvfs: 0
#    default_vf_driver: "igb_uio"
{%- else %}
#  - bus_info: "18:00.0"                    # pci bus info
#    pf_driver: {% if nic == 'cvl' %}ice{% else %}i40e{% endif %}                         # PF driver, "i40e", "ice"
{%- if ddp in ['on', 'optional'] %}
#    ddp_profile: {% if nic == 'cvl' %}"ice_comms-1.3.35.0.pkg"{% else %}gtp.pkgo{% endif %}  # DDP package name to be loaded into the NIC
                                            # For i40e(XV710-*) allowable ddp values are: "ecpri.pkg", "esp-ah.pkg", "ppp-oe-ol2tpv2.pkgo", "mplsogreudp.pkg" and "gtp.pkgo", replace as required
                                            # For ice(E810-*) allowable ddp values are: ice_comms-1.3.[17,20,22,24,28,30,31,35].0.pkg  such as "ice_comms-1.3.35.0.pkg", replace as required
                                            # ddp_profile must be defined for first port of each network device. bifurcated cards will appear as unique devices.
{% endif %}
{%- if intel_ethernet_operator.enabled in ['on', 'optional'] %}
#    flow_configuration: {% if intel_ethernet_operator.flow_config == 'on' and nic == "cvl" %}true{% else %}false{% endif %}              # Flow Configuration # NOTE: this option is for Intel E810 Series NICs and requires Intel Ethernet Operator and Flow Config to be enabled in group vars.
                                            # with Flow Configuration enabled the first VF (VF0) will be reserved for Flow Configuration and the rest of VFs will be indexed starting from 1.
{% endif %}
#    default_vf_driver: "iavf"              # default driver to be used with VFs if specific driver is not defined in the "sriov_vfs" section
#    sriov_numvfs: 6                        # total number of VFs to create including VFs listed in the "sriov_vfs" section.
                                            # If total number of VFs listed in the "sriov_vfs" section is greater than "sriov_numvfs" then excessive entities will be ignored.
                                            # VF's name should follow scheme: <arbitrary_vf_name>_<zero_started_index_of_vf>
                                            # If index in the VF's name is greater than "sriov_numfs - 1" such VF will be ignored.
{%- if minio in ['on', 'optional'] %}
#    minio_vf: true
{% endif %}
#    sriov_vfs:                             # list of VFs to create on this PF with specific driver
#      vf_00: "vfio-pci"                    # VF driver to be attached to this VF under this PF. Options: "iavf", "vfio-pci", "igb_uio"
#      vf_05: "vfio-pci"

#  - bus_info: "18:00.1"
#    pf_driver: {% if nic == 'cvl' %}ice{% else %}i40e{% endif %}
{%- if ddp in ['on', 'optional'] %}
#    ddp_profile: {% if nic == 'cvl' %}"ice_comms-1.3.35.0.pkg"{% else %}gtp.pkgo{% endif %}
{%- endif %}
#    default_vf_driver: "vfio-pci"
{%- if intel_ethernet_operator.enabled in ['on', 'optional'] %}
#    flow_configuration: {% if intel_ethernet_operator.flow_config == 'on' and nic == "cvl" %}true{% else %}false{% endif %}
{% endif %}
#    sriov_numvfs: 4
{%- if minio in ['on', 'optional'] %}
#    minio_vf: true
{% endif %}
#    sriov_vfs: {}                   # no VFs with specific driver on this PF or "sriov_vfs" can be omitted for convenience
{% endif %}

{%- if ddp in ['on', 'optional'] %}
# install Intel x700 & x800 series NICs DDP packages
install_ddp_packages: {% if ddp == 'on' and nic == 'fvl'%}true{% else %}false{% endif %}
# If following error appears: "Flashing failed: Operation not permitted"
# run deployment with update_nic_firmware: true
# or
# Disable ddp installation via install_ddp_packages: false

# set 'true' to enable custom ddp package to be loaded after reboot
enable_ice_systemd_service: {% if ddp == "on" %}true{% else %}false{% endif %}
{% endif %}

{%- if sriov_network_dp in ['on', 'optional'] %}
sriov_cni_enabled: {% if sriov_network_dp == 'on' %}true{% else %}false{% endif %}
{% endif %}
{%- endif %}

{%- if sriov_operator in ['on', 'optional'] %}
# Custom SriovNetworkNodePolicy manifests local path
# custom_sriov_network_policies_dir: /tmp/sriov
{%- endif %}

{%- if bond_cni in ['on', 'optional'] %}
# Bond CNI
bond_cni_enabled: {% if bond_cni == 'on' %}true{% else %}false{% endif %}
{% endif %}

{%- if dpdk in ['on', 'optional'] %}
# Install DPDK (required for SR-IOV networking)
install_dpdk: {% if dpdk == 'on' %}true{% else %}false{% endif %}
# DPDK version (will be in action if install_dpdk: true)
dpdk_version: "22.03"
# Custom DPDK patches local path
#dpdk_local_patches_dir: "/tmp/patches/dpdk"
# It might be necessary to adjust the patch strip parameter, update as required.
#dpdk_local_patches_strip: 0
{%- endif %}
{% if network_userspace in ['on', 'optional'] %}
# Userspace networking
userspace_cni_enabled: {% if network_userspace == 'on' %}true{% else %}false{% endif %}
ovs_dpdk_enabled: {% if ovs_dpdk == 'on' %}true{% else %}false{% endif %} # Should be enabled with Userspace CNI, when VPP is set to "false"; 1G hugepages required
ovs_version: "v2.17.1"
# CPU mask for OVS-DPDK PMD threads
ovs_dpdk_lcore_mask: 0x1
# Huge memory pages allocated by OVS-DPDK per NUMA node in megabytes
# example 1: "256,512" will allocate 256MB from node 0 and 512MB from node 1
# example 2: "1024" will allocate 1GB from node 0 on a single socket board, e.g. in a VM
ovs_dpdk_socket_mem: "256,0"
vpp_enabled: {% if vpp == 'on'%}true{% else %}false{% endif %} # Should be enabled with Userspace CNI, when ovs_dpdk is set to "false"; 2M hugepages required
{% endif %}

{%- if hugepages in ['on', 'optional'] %}
# Enables hugepages support
hugepages_enabled: {% if hugepages == 'on' %}true{% else %}false{% endif %}
# Hugepage sizes available: 2M, 1G
default_hugepage_size: {% if vpp == 'on' %}2M{% else %}1G{% endif %}
# Sets how many hugepages should be created
number_of_hugepages_1G: 4
number_of_hugepages_2M: 1024
{% endif %}

{%- if intel_ethernet_operator.enabled in ['on', 'optional'] %}
# Intel Ethernet Operator for Intel E810 Series network interface cards
intel_ethernet_operator:
{%- if intel_ethernet_operator.ddp in ['on', 'optional'] %}
  ddp_update: {% if intel_ethernet_operator.ddp == 'on' and nic == 'cvl' %}true{% else %}false{% endif %}                   # perform DDP update on PFs listed in dataplane_interfaces using selected DDP profile
{%- endif %}
  fw_update: {% if intel_ethernet_operator.fw_update == 'on' and nic == 'cvl' %}true{% else %}false{% endif %}                     # perform firmware update on PFs listed in dataplane_interfaces
  # NodeFlowConfig manifests local path
  # For more information refer to:
  # https://github.com/intel/intel-ethernet-operator/blob/main/docs/flowconfig-daemon/creating-rules.md
  # node_flow_config_dir: /tmp/node_flow_config
{% endif %}

{%- if intel_sriov_fec_operator in ['on', 'optional'] %}
# Wireless FEC H/W Accelerator Device (e.g. ACC100) PCI ID
fec_acc: "0000:27:00.0"  # must be string in [a-fA-F0-9]{4}:[a-fA-F0-9]{2}:[01][a-fA-F0-9].[0-7] format
{% endif %}

{%- if intel_flexran in ['on', 'optional'] %}
# Intel FlexRAN
intel_flexran_enabled: {% if intel_flexran == 'on' %}true{% else %}false{% endif %} # if true, deploy FlexRAN
{% endif %}

{%- if qat in ['on', 'optional'] %}
# Enabling this feature will install QAT drivers + services
update_qat_drivers: {% if qat == "on" %}true{% else %}false{% endif %}

# qat interface configuration list
qat_devices: []
{%- if on_vms == 'on' %}
#  - qat_id: "0000:0a:00.0"
#    qat_sriov_numvfs: 0              # Have to be set to 0 here to not create any VFs inside VM.

#  - qat_id: "0000:0b:00.0"
#    qat_sriov_numvfs: 0              # Have to be set to 0 here to not create any VFs inside VM.
{%- else %}
#  - qat_id: "0000:ab:00.0"           # QAT device id one using DPDK compatible driver for VF devices to be used by vfio-pci kernel driver, replace as required
#    qat_sriov_numvfs: 12             # Number of VFs per PF to create - cannot exceed the maximum number of VFs available for the device. Set to 0 to not create any VFs.
                                      # Note: Currently when trying to create fewer virtual functions than the maximum, the maximum number always gets created
#  - qat_id: "0000:xy:00.0"
#    qat_sriov_numvfs: 10

#  - qat_id: "0000:yz:00.0"
#    qat_sriov_numvfs: 10
{%- endif %}
{% endif %}

{%- if openssl in ['on', 'optional'] %}
# Install and configure OpenSSL cryptography
openssl_install: {% if openssl == 'on' and qat == "on" %}true{% else %}false{% endif %} # This requires update_qat_drivers set to 'true' in host vars
{% endif %}

{%- if isolcpu in ["on", "optional"] %}
# CPU isolation from Linux scheduler
isolcpus_enabled: {% if isolcpu == 'on' %}true{% else %}false{% endif %}
{%- if on_vms == 'on' %}
isolcpus: "4-15"
{%- else %}
isolcpus: "4-11"
{%- endif %}
{% endif %}

{%- if cpusets in ["on", "optional"] %}
# CPU shielding
cpusets_enabled: {% if cpusets == 'on' %}true{% else %}false{% endif %}
{%- if on_vms == 'on' %}
cpusets: "4-15"
{%- else %}
cpusets: "4-11"
{%- endif %}
{% endif %}


{%- if native_cpu_manager in ["on", "optional"] %}
# Native CPU Manager (Kubernetes built-in)
# These settings are relevant only if in group_vars native_cpu_manager_enabled: true
# Amount of CPU cores that will be reserved for the housekeeping (2000m = 2000 millicores = 2 cores)
native_cpu_manager_system_reserved_cpus: 2000m
# Amount of CPU cores that will be reserved for Kubelet
native_cpu_manager_kube_reserved_cpus: 1000m
# Explicit list of the CPUs reserved for the host level system threads and Kubernetes related threads
#native_cpu_manager_reserved_cpus: "0,1,2"
# Note: All remaining unreserved CPU cores will be consumed by the workloads.
{% endif %}
{%- if (pstate in ['on', 'optional'] or sst in ['on', 'optional']) and arch in ['clx', 'icx', 'spr'] %}
# Enable/Disable Intel PState scaling driver
intel_pstate_enabled: {% if pstate == "on" or sst == "on" %}true{% else %}false{% endif %}
# Config options for intel_pstate: disable, passive, force, no_hwp, hwp_only, support_acpi_ppc, per_cpu_perf_limits
intel_pstate: {% if pstate == "on" or sst == "on" %}hwp_only{% else %}disable{% endif %}
# Enable/Disable Intel Turbo Boost PState attribute
turbo_boost_enabled: {% if on_vms != 'on' %}true{% else %}false{% endif %}
{% endif -%}

{% if cstate in ['on', 'optional'] %}
cstate_enabled: {% if cstate == "on" %}true{% else %}false{% endif %}
cstates:
{%- if name == 'access' %}
  C6: # default values: C6 for access, C1 for other profiles
    cpu_range: '0-9' # change as needed, cpus to modify cstates on
    enable: true # true - enable given cstate, false - disable given cstate
{%- else %}
  C1: # default values: C6 for access, C1 for other profiles
    cpu_range: '0-9' # change as needed, cpus to modify cstates on
    enable: true # true - enable given cstate, false - disable given cstate
{% endif -%}
{% endif -%}

{% if ufs in ['on', 'optional'] %}
ufs_enabled: {% if ufs == "on" %}true{% else %}false{% endif %}
ufs: # uncore frequency scaling
  min: 1000 # minimal uncore frequency
  max: 2000 # maximal uncore frequency
{% endif -%}

{% if sst in ['on', 'optional'] %}
{%- if arch in ['icx', 'spr'] %}
# Intel(R) SST-PP (perf-profile) configuration
# [true] Enable Intel(R) SST-PP (perf-profile)
# [false] Disable Intel(R) SST-PP (perf-profile)
sst_pp_configuration_enabled: {% if sst == "on" %}true{% else %}false{% endif %}
sst_pp_config_list:             # "enable" or "disable" list options per SST-PP setup requirements
    - sst_bf: "enable"          # "enable" or "disable" Intel(R) SST-BF (base-freq) to configure with SST-PP
    - sst_cp: "enable"          # "enable" or "disable" Intel(R) SST-CP (core-power) to configure with SST-PP.
    - sst_tf: "enable"          # "enable" or "disable" Intel(R) SST-TF (turbo-freq) to configure with SST-PP.
      online_cpus_range: "auto" # "auto" will config turbo-freq for all available online CPUs or else define specific CPUs such as "2,3,5" to prioritize among others.
{% endif %}
# Intel Speed Select Base-Frequency configuration.
{%- if arch == 'clx' %}
sst_bf_configuration_enabled: {% if sst == "on" %}true{% else %}false{% endif %}
# Intel Speed Select Base-Frequency configuration for Cascade Lake (CLX) Platforms.
# CLX support of SST-BF requires 'intel_pstate' to be 'enabled'
# Option clx_sst_bf_mode requires sst_bf_configuration_enabled to be set to 'true'.
# There are three configuration modes:
# [s] Set SST-BF config (set min/max to 2700/2700 and 2100/2100)
# [m] Set P1 on all cores (set min/max to 2300/2300)
# [r] Revert cores to min/Turbo (set min/max to 800/3900)
clx_sst_bf_mode: s
{%- endif %}
{%- if arch == 'icx' %}
# Intel Speed Select Base-Frequency configuration for Ice Lake (ICX) Platforms.
# [true] Enable Intel Speed Select Base Frequency (SST-BF)
# [false] Disable Intel Speed Select Base Frequency (SST-BF)
# Requires `sst_bf_configuration_enabled` variable to be 'true'
icx_sst_bf_enabled: {% if sst == "on" %}true{% else %}false{% endif %}
# Prioritze (SST-CP) power flow to high frequency cores in case of CPU power constraints.
icx_sst_bf_with_core_priority: {% if sst == "on" %}true{% else %}false{% endif %}

# SST CP config
# Variables are only examples.
# For more information, please visit:
# https://www.kernel.org/doc/html/latest/admin-guide/pm/intel-speed-select.html#enable-clos-based-prioritization
# Enabling this configuration overrides `icx_sst_bf_with_core_priority`.
sst_cp_configuration_enabled: {% if sst == "on" %}true{% else %}false{% endif %}
sst_cp_priority_type: "1" # For Proportional select "0" and for Ordered select "1", update as required
sst_cp_clos_groups: # configure up to 4 CLOS groups
  - id: 0
    frequency_weight: 0 # used only with Proportional type
    min_MHz: 0
    max_MHz: 25500
  - id: 1
    frequency_weight: 0 # used only with Proportional type
    min_MHz: 0
    max_MHz: 25500
  - id: 2
    frequency_weight: 0 # used only with Proportional type
    min_MHz: 0
    max_MHz: 25500
  - id: 3
    frequency_weight: 0 # used only with Proportional type
    min_MHz: 0
    max_MHz: 25500
sst_cp_cpu_clos: # assign required values to CLOS group after priority type setup
  - clos: 0      # associating CPUs with a CLOS group such as "clos: 3", update as required.
    cpus: 2,3,5  # define specific CPUs per CLOS group such as "cpus: 2,6,9", update as required.  
  - clos: 1
    cpus: 12     

# Intel(R) SST-TF (feature turbo-freq) configuration for Ice Lake (ICX) Platforms.
# [true] Enable Intel Speed Select Turbo Frequency (SST-TF)
# [false] Disable Intel Speed Select Turbo Frequency (SST-TF)
sst_tf_configuration_enabled: {% if sst == "on" %}true{% else %}false{% endif %}
{%- endif %}
{% endif %}

{%- if sgx in ['on', 'optional'] and arch in ['icx', 'spr'] %}
# Intel Software Guard Extensions (SGX)
configure_sgx: {% if sgx == 'on' %}true{% else %}false{% endif %}
{% endif %}

{%- if gpu in ['on', 'optional'] %}
# Intel custom GPU kernel - this is required to be true in order to
# deploy Intel GPU Device Plugin on that node
configure_gpu: {% if gpu == 'on' %}true{% else %}false{% endif %}
{% endif %}

{%- if telemetry.collectd in ['on', 'optional'] %}
# Telemetry configuration
# intel_pmu plugin collects information provided by Linux perf interface.
enable_intel_pmu_plugin: false

{%- if on_vms == 'on' %}

# Temporary Fix for collectd startup issue on VMs
enable_pkgpower_plugin: false
{%- endif %}

# CPU Threads to be monitored by Intel PMU Plugin.
# If the field is empty, all available cores will be monitored.
# Please refer to https://collectd.org/wiki/index.php/Plugin:Intel_PMU for configuration details.
intel_pmu_plugin_monitored_cores: ""

# CPU Threads to be monitored by Intel RDT Plugin.
# If the field is empty, all available cores will be monitored.
# Please refer to https://collectd.org/wiki/index.php/Plugin:IntelRDT for configuration details.
intel_rdt_plugin_monitored_cores: ""

# Additional list of plugins that will be excluded from collectd deployment.
exclude_collectd_plugins: []
{% endif %}

{%- if cndp in ['on', 'optional'] %}
# Intel Cloud Native Data Plane.
cndp_enabled: {% if cndp == 'on' %}true{% else %}false{% endif %}
{%- if cndp_dp in ['on', 'optional'] %}
cndp_dp_pools:
  - name: "e2e"
    drivers: {% raw %}"{{ dataplane_interfaces | map(attribute='pf_driver') | list | unique }}"{% endraw %}   # List of NIC driver to be included in CNDP device plugin ConfigMap.
{% endif %}
{% endif %}

{%- if vm_mode in ['on'] and on_vms != 'on' %}
# The only common VM image for all VMs inside deployment is supported at the moment
#
{%- if secondary_host == 'true' %}
# Do not set VM image info here - do it just on the first vm_host
# Secondary vm_host - do not change dhcp settings here
dhcp: []
{% else %}
# Default VM image version is Ubuntu 20.04 - focal
# Supported VM image distributions ['ubuntu', 'rocky']
#vm_image_distribution: "ubuntu"
# Supported VM image ubuntu versions ['20.04', '22.04']
#vm_image_version_ubuntu: "22.04"
# Supported VM image rocky versions ['8.5']
#vm_image_version_rocky: "8.5"
# dhcp for vxlan have to be enabled just on the first vm_host
dhcp:
  - 120
#vxlan_gw_ip: "40.0.0.1/24"
{% endif -%}
# Set hashed password for root user inside VMs. Current value is just placeholder.
# To create hashed password use e.g.: openssl passwd -6 -salt SaltSalt <your_password>
vm_hashed_passwd: 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'
vxlan_device: eno2
#cpu_host_os will change number of CPUs reserved for host OS. Default value is 16
#cpu_host_os: 8
vms:
{%- if secondary_host == 'true' %}
#  - type: "ctrl"
#    name: "vm-ctrl-1"
#    cpu_total: 8
#    memory: 20480
#    vxlan: 120
{% else %}
  - type: "ctrl"
    name: "vm-ctrl-1"
    # Parameter cpus and numa can be uncommented and specified manually, but by default
    # are CPUs and NUMA node allocated automatically.
    # cpus: "8-11,64-67"
    # numa: 0
    # Parameter emu_cpus is DEPRECATED, uncommenting line below would not have
    # any impact, as emulators CPUs are picked-up automatically.
    # emu_cpus: "8,64"
    # if you set cpu_total: to 0 then rest of unallocated CPUs from selected numa will be used
    cpu_total: 8
    # if 'alloc_all: true' is used then 'cpu_total' have to be set to '0'
    # It will take all unallocated CPUs from all NUMA nodes on the vm_host.
    # alloc_all: true
    memory: 20480
    vxlan: 120
{% endif -%}
#  - type: "ctrl"
#    name: "vm-ctrl-2"
#    cpu_total: 8
#    memory: 20480
#    vxlan: 120
#  - type: "ctrl"
#    name: "vm-ctrl-3"
#    cpu_total: 8
#    memory: 20480
#    vxlan: 120
{%- if secondary_host == 'true' %}
#  - type: "work"
#    name: "vm-work-1"
#    cpu_total: 16
#    memory: 61440
#    vxlan: 120
{%- if name not in ['build_your_own'] %}
#    pci:
#      - "18:02.2"
#      - "18:02.3"
#      - "18:02.4"
#      - "18:02.5"
{%- if qat == "on" %}
##      - "3d:01.1"
##      - "3f:01.1"
{%- endif %}
{%- else %}
#    pci: []
{%- endif %}
{% else %}
  - type: "work"
    name: "vm-work-1"
    # Parameter cpus and numa can be uncommented and specified manually, but by default
    # are CPUs and NUMA node allocated automatically.
    # cpus: "28-35,84-91"
    # numa: 1
    # Parameter emu_cpus is DEPRECATED, uncommenting line below would not have
    # any impact, as emulators CPUs are picked-up automatically.
    # emu_cpus: "28,84"
    # if you set cpu_total: to 0 then rest of unallocated CPUs from selected numa will be used
    cpu_total: 16
    # if 'alloc_all: true' is used then 'cpu_total' have to be set to '0'
    # It will take all unallocated CPUs from all NUMA nodes on the vm_host.
    # alloc_all: true
    memory: 61440
    vxlan: 120
{%- if name not in ['build_your_own'] %}
    pci:
      - "18:02.2"
      - "18:02.3"
      - "18:02.4"
      - "18:02.5"
{%- if qat == "on" %}
#      - "3d:01.1"
#      - "3f:01.1"
{%- endif %}
{%- else %}
    pci: []
{%- endif %}
{% endif -%}
#  - type: "work"
#    name: "vm-work-2"
#    cpu_total: 16
#    memory: 61440
#    vxlan: 120
{%- if name not in ['build_your_own'] %}
#    pci:
#      - "18:02.6"
#      - "18:02.6"
#      - "18:02.0"
#      - "18:02.6"
{%- if qat == "on" %}
##      - "3d:01.2"
##      - "3f:01.2"
{%- endif %}
{%- else %}
#    pci: []
{%- endif %}
{% endif -%}
{%- if power_manager in ['on', 'optional'] and arch in ['icx', 'clx', 'spr'] -%}
# Power Operator Shared Profile/Workload settings.
# It is possible to create node-specific Power Profile
local_shared_profile:
  enabled: false
  node_max_shared_frequency: 2000
  node_min_shared_frequency: 1500

# Shared Workload is required to make use of Shared Power Profile
shared_workload:
  enabled: false
  reserved_cpus: []   # The CPUs in reserved_cpus should match the value of the reserved system CPUs in your Kubelet config file, if none please
                      # set here a dummy core - the last one to avoid AppQos bug
  shared_workload_type: "global"  # set to node name to make use of node-specific Power Profile, 'global' means use cluster-specific custom Power Profile
{% endif %}
{%- if minio in ['on', 'optional'] %}
# MinIO storage configuration
minio_pv: []
#  - name: "mnt-data-1"                         # PV identifier will be used for PVs names followed by node name(e.g., mnt-data-1-hostname)
#    storageClassName: "local-storage"          # Storage class name to match with PVC
#    accessMode: "ReadWriteOnce"                # Access mode when mounting a volume, e.g., ReadWriteOnce/ReadOnlyMany/ReadWriteMany/ReadWriteOncePod
#    persistentVolumeReclaimPolicy: "Retain"    # Reclaim policy when a volume is released once it's bound, e.g., Retain/Recycle/Delete
#    capacity: 1GiB                             # Size of the PV. support only GiB/TiB
#    mountPath: /mnt/data0                      # Mount path of a volume
#    device: /dev/nvme0n1                       # Target storage device name when creating a volume.
                                                # When group_vars: minio_deploy_test_mode == true, use a file as a loop device for storage
                                                # otherwise, an actual NVME or SSD device for storage on the device name.

#  - name: "mnt-data-2"
#    storageClassName: "local-storage"
#    accessMode: "ReadWriteOnce"
#    persistentVolumeReclaimPolicy: "Retain"
#    capacity: 1GiB
#    mountPath: /mnt/data1
#    device: /dev/nvme1n1

#  - name: "mnt-data-3"
#    storageClassName: "local-storage"
#    accessMode: "ReadWriteOnce"
#    persistentVolumeReclaimPolicy: "Retain"
#    capacity: 1GiB
#    mountPath: /mnt/data2
#    device: /dev/nvme2n1

#  - name: "mnt-data-4"
#    storageClassName: "local-storage"
#    accessMode: "ReadWriteOnce"
#    persistentVolumeReclaimPolicy: "Retain"
#    capacity: 1GiB
#    mountPath: /mnt/data3
#    device: /dev/nvme3n1
{% endif -%}
